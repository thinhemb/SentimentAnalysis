{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Trainer.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"onry4ml0zJK7","outputId":"fd870c2b-48bf-42c2-c525-b20f1cad7e7f","executionInfo":{"status":"ok","timestamp":1649205252688,"user_tz":-420,"elapsed":18673,"user":{"displayName":"Đỗ Thịnh","userId":"08305213904978546184"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mp24hTdwzKo3","outputId":"aa5bfa82-dae3-45c1-95cb-603a54a7c663","executionInfo":{"status":"ok","timestamp":1649205253076,"user_tz":-420,"elapsed":397,"user":{"displayName":"Đỗ Thịnh","userId":"08305213904978546184"}}},"source":["%cd /content/drive/MyDrive/SentimentAnalysis/main"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1W4JrmjZE9gvw_ESeKYn9vIXIAGhPxOQC/SentimentAnalysis/main\n"]}]},{"cell_type":"code","metadata":{"id":"kY4eu8Af0JkI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"50de071c-3e7e-4045-a10e-f32048c51f9b","executionInfo":{"status":"ok","timestamp":1648905149594,"user_tz":-420,"elapsed":2121,"user":{"displayName":"61.Đỗ Viết Thịnh","userId":"09066806918916879906"}}},"source":["# !git clone https://github.com/huggingface/transformers\n","%cd transformers\n","!pip install ."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'transformers' already exists and is not an empty directory.\n","/content/drive/MyDrive/SentimentAnalysis/main/transformers\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"gvBiibEUzq3V","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"c6cba84c-c673-45e4-bdb3-fc16e6ba1f33","executionInfo":{"status":"ok","timestamp":1649205305686,"user_tz":-420,"elapsed":52296,"user":{"displayName":"Đỗ Thịnh","userId":"08305213904978546184"}}},"source":["!pip install clearml numpy clearml-agent 'pyjwt<1.8.0'\n","!pip install transformers\n","!pip install sentencepiece\n","!pip install fairseq\n","!pip install tensorboardX\n","!pip install fastBPE\n","!pip install vncorenlp"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting clearml\n","  Downloading clearml-1.3.2-py2.py3-none-any.whl (780 kB)\n","\u001b[K     |████████████████████████████████| 780 kB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n","Collecting clearml-agent\n","  Downloading clearml_agent-1.2.2-py3-none-any.whl (371 kB)\n","\u001b[K     |████████████████████████████████| 371 kB 27.9 MB/s \n","\u001b[?25hCollecting pyjwt<1.8.0\n","  Downloading PyJWT-1.7.1-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from clearml) (3.0.7)\n","Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from clearml) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from clearml) (2.8.2)\n","Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from clearml) (3.13)\n","Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from clearml) (2.23.0)\n","Collecting pathlib2>=2.3.0\n","  Downloading pathlib2-2.3.7.post1-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from clearml) (1.15.0)\n","Requirement already satisfied: jsonschema>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from clearml) (4.3.3)\n","Requirement already satisfied: attrs>=18.0 in /usr/local/lib/python3.7/dist-packages (from clearml) (21.4.0)\n","Requirement already satisfied: psutil>=3.4.2 in /usr/local/lib/python3.7/dist-packages (from clearml) (5.4.8)\n","Requirement already satisfied: Pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from clearml) (7.1.2)\n","Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from clearml) (1.24.3)\n","Collecting furl>=2.0.0\n","  Downloading furl-2.1.3-py2.py3-none-any.whl (20 kB)\n","Collecting orderedmultidict>=1.0.1\n","  Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6.0->clearml) (3.10.0.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6.0->clearml) (4.11.3)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6.0->clearml) (0.18.1)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6.0->clearml) (5.4.0)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6.0->clearml) (3.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->clearml) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->clearml) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->clearml) (3.0.4)\n","Collecting pyparsing>=2.0.3\n","  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 5.9 MB/s \n","\u001b[?25hCollecting virtualenv<21,>=16\n","  Downloading virtualenv-20.14.0-py2.py3-none-any.whl (8.8 MB)\n","\u001b[K     |████████████████████████████████| 8.8 MB 43.7 MB/s \n","\u001b[?25hCollecting attrs>=18.0\n","  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 5.6 MB/s \n","\u001b[?25hCollecting jsonschema>=2.6.0\n","  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n","\u001b[K     |████████████████████████████████| 56 kB 4.0 MB/s \n","\u001b[?25hCollecting pyhocon<0.4.0,>=0.3.38\n","  Downloading pyhocon-0.3.59.tar.gz (116 kB)\n","\u001b[K     |████████████████████████████████| 116 kB 50.2 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6.0->clearml) (57.4.0)\n","Collecting distlib<1,>=0.3.1\n","  Downloading distlib-0.3.4-py2.py3-none-any.whl (461 kB)\n","\u001b[K     |████████████████████████████████| 461 kB 46.7 MB/s \n","\u001b[?25hRequirement already satisfied: filelock<4,>=3.2 in /usr/local/lib/python3.7/dist-packages (from virtualenv<21,>=16->clearml-agent) (3.6.0)\n","Collecting platformdirs<3,>=2\n","  Downloading platformdirs-2.5.1-py3-none-any.whl (14 kB)\n","Building wheels for collected packages: pyhocon\n","  Building wheel for pyhocon (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyhocon: filename=pyhocon-0.3.59-py3-none-any.whl size=19968 sha256=89f95fa9cd53f25aaba908b4f877db535b55e40b1ea4d67347b17782c1026cc9\n","  Stored in directory: /root/.cache/pip/wheels/69/f4/73/2afd609de4a040ee997bb9026d145e626124bc3ce8e5c23f79\n","Successfully built pyhocon\n","Installing collected packages: pyparsing, platformdirs, orderedmultidict, distlib, attrs, virtualenv, pyjwt, pyhocon, pathlib2, jsonschema, furl, clearml-agent, clearml\n","  Attempting uninstall: pyparsing\n","    Found existing installation: pyparsing 3.0.7\n","    Uninstalling pyparsing-3.0.7:\n","      Successfully uninstalled pyparsing-3.0.7\n","  Attempting uninstall: attrs\n","    Found existing installation: attrs 21.4.0\n","    Uninstalling attrs-21.4.0:\n","      Successfully uninstalled attrs-21.4.0\n","  Attempting uninstall: jsonschema\n","    Found existing installation: jsonschema 4.3.3\n","    Uninstalling jsonschema-4.3.3:\n","      Successfully uninstalled jsonschema-4.3.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","nbclient 0.5.13 requires jupyter-client>=6.1.5, but you have jupyter-client 5.3.5 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed attrs-20.3.0 clearml-1.3.2 clearml-agent-1.2.2 distlib-0.3.4 furl-2.1.3 jsonschema-3.2.0 orderedmultidict-1.0.1 pathlib2-2.3.7.post1 platformdirs-2.5.1 pyhocon-0.3.59 pyjwt-1.7.1 pyparsing-2.4.7 virtualenv-20.14.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pyparsing"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 5.3 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.0-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 6.5 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 50.2 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Collecting tokenizers!=0.11.3,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 38.2 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 45.8 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","clearml-agent 1.2.2 requires PyYAML<5.5.0,>=3.12, but you have pyyaml 6.0 which is incompatible.\u001b[0m\n","Successfully installed huggingface-hub-0.5.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 5.2 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n","Collecting fairseq\n","  Downloading fairseq-0.10.2-cp37-cp37m-manylinux1_x86_64.whl (1.7 MB)\n","\u001b[K     |████████████████████████████████| 1.7 MB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.15.0)\n","Collecting dataclasses\n","  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq) (0.29.28)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq) (4.63.0)\n","Collecting sacrebleu>=1.4.12\n","  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n","\u001b[K     |████████████████████████████████| 90 kB 9.0 MB/s \n","\u001b[?25hCollecting hydra-core\n","  Downloading hydra_core-1.1.1-py3-none-any.whl (145 kB)\n","\u001b[K     |████████████████████████████████| 145 kB 45.7 MB/s \n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.10.0+cu111)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.21.5)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.8.9)\n","Collecting portalocker\n","  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n","Collecting colorama\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq) (2.21)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq) (5.4.0)\n","Collecting antlr4-python3-runtime==4.8\n","  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 47.1 MB/s \n","\u001b[?25hCollecting omegaconf==2.1.*\n","  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n","\u001b[K     |████████████████████████████████| 74 kB 3.4 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from omegaconf==2.1.*->hydra-core->fairseq) (6.0)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core->fairseq) (3.7.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->fairseq) (3.10.0.2)\n","Building wheels for collected packages: antlr4-python3-runtime\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=ad9f2f8636db8806a24e714ddb74c25b329e6672d0ae99e16068d185d4221083\n","  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n","Successfully built antlr4-python3-runtime\n","Installing collected packages: antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, dataclasses, fairseq\n","Successfully installed antlr4-python3-runtime-4.8 colorama-0.4.4 dataclasses-0.6 fairseq-0.10.2 hydra-core-1.1.1 omegaconf-2.1.1 portalocker-2.4.0 sacrebleu-2.0.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pydevd_plugins"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting tensorboardX\n","  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n","\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 20.8 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 92 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.15.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.5)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.5\n","Collecting fastBPE\n","  Downloading fastBPE-0.1.0.tar.gz (35 kB)\n","Building wheels for collected packages: fastBPE\n","  Building wheel for fastBPE (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fastBPE: filename=fastBPE-0.1.0-cp37-cp37m-linux_x86_64.whl size=483161 sha256=7e283d8248f0b6f2939b739f6fee170b8c4442d77a542bc9ead0037a1482f258\n","  Stored in directory: /root/.cache/pip/wheels/bd/d4/0e/0d317a65f77d3f8049fedd8a2ee0519164cf3e6bd77ef886f1\n","Successfully built fastBPE\n","Installing collected packages: fastBPE\n","Successfully installed fastBPE-0.1.0\n","Collecting vncorenlp\n","  Downloading vncorenlp-1.0.3.tar.gz (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vncorenlp) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (1.24.3)\n","Building wheels for collected packages: vncorenlp\n","  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-py3-none-any.whl size=2645951 sha256=0f49d827a8caf297254463493c99a88fabb50ae335b6d5a17b85c1d9927ac758\n","  Stored in directory: /root/.cache/pip/wheels/0c/d8/f2/d28d97379b4f6479bf51247c8dfd57fa00932fa7a74b6aab29\n","Successfully built vncorenlp\n","Installing collected packages: vncorenlp\n","Successfully installed vncorenlp-1.0.3\n"]}]},{"cell_type":"code","metadata":{"id":"kvdCCzmP0Vfa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0e5b6bae-3512-4973-cb8e-fe72d332c0e7","executionInfo":{"status":"ok","timestamp":1649037191510,"user_tz":-420,"elapsed":3113,"user":{"displayName":"61.Đỗ Viết Thịnh","userId":"09066806918916879906"}}},"source":["!pip install vncorenlp\n","# Download VnCoreNLP-1.1.1.jar & its word segmentation component (i.e. RDRSegmenter) \n","# !mkdir -p vncorenlp/models/wordsegmenter\n","# !wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n","# !wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n","# !wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n","# !mv VnCoreNLP-1.1.1.jar vncorenlp/ \n","# !mv vi-vocab vncorenlp/models/wordsegmenter/\n","# !mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: vncorenlp in /usr/local/lib/python3.7/dist-packages (1.0.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vncorenlp) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2021.10.8)\n"]}]},{"cell_type":"code","metadata":{"id":"3dSM72_L0paR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648869280166,"user_tz":-420,"elapsed":30924,"user":{"displayName":"61.Đỗ Viết Thịnh","userId":"09066806918916879906"}},"outputId":"af49c573-6b41-4dc2-f0b6-661dc20089cc"},"source":["!wget https://public.vinai.io/PhoBERT_base_transformers.tar.gz\n","!tar -xzvf PhoBERT_base_transformers.tar.gz"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-04-02 03:14:08--  https://public.vinai.io/PhoBERT_base_transformers.tar.gz\n","Resolving public.vinai.io (public.vinai.io)... 143.204.231.117, 143.204.231.42, 143.204.231.27, ...\n","Connecting to public.vinai.io (public.vinai.io)|143.204.231.117|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 322405979 (307M) [application/x-tar]\n","Saving to: ‘PhoBERT_base_transformers.tar.gz’\n","\n","PhoBERT_base_transf 100%[===================>] 307.47M  28.4MB/s    in 14s     \n","\n","2022-04-02 03:14:26 (22.5 MB/s) - ‘PhoBERT_base_transformers.tar.gz’ saved [322405979/322405979]\n","\n","PhoBERT_base_transformers/\n","PhoBERT_base_transformers/config.json\n","PhoBERT_base_transformers/bpe.codes\n","PhoBERT_base_transformers/model.bin\n","PhoBERT_base_transformers/dict.txt\n"]}]},{"cell_type":"code","source":["%%bash\n","\n","export api=`cat <<EOF\n","api { \n","    # Đỗ Viết Thịnh's workspace\n","    web_server: https://app.clear.ml\n","    api_server: https://api.clear.ml\n","    files_server: https://files.clear.ml\n","    # colab\n","    credentials {\n","        \"access_key\" = \"PUFDOMYFYXQYI2X66STJ\"\n","        \"secret_key\" = \"S6XKiHuk4W6SQPktBsNpJDwoIW77E5PkKbir4iJGRfoXf5JuRE\"\n","    }\n","}\n","EOF\n","`\n","echo \"$api\" > /root/clearml.conf"],"metadata":{"id":"XQwVA88fNyGZ","executionInfo":{"status":"ok","timestamp":1649205305687,"user_tz":-420,"elapsed":26,"user":{"displayName":"Đỗ Thịnh","userId":"08305213904978546184"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from clearml import Task\n","\n","task = Task.init(project_name='SA_comment_VN', task_name='task_2')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N6dsSZ2fOf0f","executionInfo":{"status":"ok","timestamp":1649205315574,"user_tz":-420,"elapsed":9910,"user":{"displayName":"Đỗ Thịnh","userId":"08305213904978546184"}},"outputId":"b99ef410-b31d-4067-b365-252809ff6882"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["ClearML Task: created new task id=2cbf865946aa4e6ea88e7ac6919efe36\n","ClearML results page: https://app.clear.ml/projects/d022e8e14ee24ff89915feb8d8259385/experiments/2cbf865946aa4e6ea88e7ac6919efe36/output/log\n"]}]},{"cell_type":"code","metadata":{"id":"t74J0mhszZNO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0e65a53e-aa6a-4f38-948f-64988bd6965d"},"source":["!python /content/drive/MyDrive/SentimentAnalysis/main/train.py"],"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["======> WARNING! Git diff to large to store (19240kb), skipping uncommitted changes <======\n","Loading codes from /content/drive/MyDrive/SentimentAnalysis/main/transformers/PhoBERT_base_transformers/bpe.codes ...\n","Read 64000 codes from the codes file.\n","You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n","2022-04-06 07:37:45,876 - clearml.model - INFO - Selected model id: 37d4a89dbae74ad383cd5c4e5b155ad8\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","======================== Epoch 1 / 8 ======================== \n","============Training============\n","2268it [20:57,  1.80it/s]\n","Loss:    0.0001\t Average training loss:     0.0847\n"," Accuracy:     0.9828\t F1 score:     0.9744\n","============Running Validation============\n","100% 252/252 [00:38<00:00,  6.55it/s]\n"," Accuracy:     0.8358 \tF1 score:     0.7794\n","======================== Epoch 2 / 8 ======================== \n","============Training============\n","2268it [20:58,  1.80it/s]\n","Loss:    0.0014\t Average training loss:     0.0465\n"," Accuracy:     0.9909\t F1 score:     0.9863\n","============Running Validation============\n","100% 252/252 [00:38<00:00,  6.55it/s]\n"," Accuracy:     0.8309 \tF1 score:     0.7814\n","======================== Epoch 3 / 8 ======================== \n","============Training============\n","2268it [20:59,  1.80it/s]\n","Loss:    0.0000\t Average training loss:     0.0418\n"," Accuracy:     0.9918\t F1 score:     0.9863\n","============Running Validation============\n","100% 252/252 [00:38<00:00,  6.50it/s]\n"," Accuracy:     0.8452 \tF1 score:     0.7975\n","======================== Epoch 4 / 8 ======================== \n","============Training============\n","2268it [21:08,  1.79it/s]\n","Loss:    0.0000\t Average training loss:     0.0506\n"," Accuracy:     0.9908\t F1 score:     0.9857\n","============Running Validation============\n","100% 252/252 [00:38<00:00,  6.51it/s]\n"," Accuracy:     0.8403 \tF1 score:     0.7887\n","======================== Epoch 5 / 8 ======================== \n","============Training============\n","2268it [21:05,  1.79it/s]\n","Loss:    0.0000\t Average training loss:     0.0455\n"," Accuracy:     0.9905\t F1 score:     0.9845\n","============Running Validation============\n","100% 252/252 [00:38<00:00,  6.56it/s]\n"," Accuracy:     0.8363 \tF1 score:     0.7877\n","======================== Epoch 6 / 8 ======================== \n","============Training============\n","2268it [20:55,  1.81it/s]\n","Loss:    0.0000\t Average training loss:     0.0437\n"," Accuracy:     0.9913\t F1 score:     0.9874\n","============Running Validation============\n","100% 252/252 [00:38<00:00,  6.54it/s]\n"," Accuracy:     0.8328 \tF1 score:     0.7768\n","======================== Epoch 7 / 8 ======================== \n","============Training============\n","2268it [20:57,  1.80it/s]\n","Loss:    0.0000\t Average training loss:     0.0435\n"," Accuracy:     0.9909\t F1 score:     0.9863\n","============Running Validation============\n","100% 252/252 [00:38<00:00,  6.55it/s]\n"," Accuracy:     0.8398 \tF1 score:     0.7859\n","======================== Epoch 8 / 8 ======================== \n","============Training============\n","2268it [20:57,  1.80it/s]\n","Loss:    0.0001\t Average training loss:     0.0510\n"," Accuracy:     0.9898\t F1 score:     0.9843\n","============Running Validation============\n","100% 252/252 [00:38<00:00,  6.54it/s]\n"," Accuracy:     0.8487 \tF1 score:     0.8008\n","Training complete!\n"]}]},{"cell_type":"code","metadata":{"id":"IIVHjU_rePfH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9e3cc292-e5ad-496a-d0f4-f15cf219a976"},"source":["!python train.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-06-06 10:25:46.276084: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","Loading codes from transformers/PhoBERT_base_transformers/bpe.codes ...\n","Read 64000 codes from the codes file.\n","You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n","Some weights of the model checkpoint at transformers/PhoBERT_base_transformers/model.bin were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at transformers/PhoBERT_base_transformers/model.bin and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","======== Epoch 1 / 10 ========\n","Training...\n","1680it [07:28,  3.74it/s]\n"," Accuracy: 0.9187\n"," F1 score: 0.8849\n"," Average training loss: 0.3235\n","Running Validation...\n","100% 560/560 [00:37<00:00, 14.86it/s]\n"," Accuracy: 0.7645\n"," F1 score: 0.6912\n","======== Epoch 2 / 10 ========\n","Training...\n","1680it [07:36,  3.68it/s]\n"," Accuracy: 0.9525\n"," F1 score: 0.9286\n"," Average training loss: 0.2103\n","Running Validation...\n","100% 560/560 [00:37<00:00, 14.83it/s]\n"," Accuracy: 0.7745\n"," F1 score: 0.7004\n","======== Epoch 3 / 10 ========\n","Training...\n","1680it [07:35,  3.68it/s]\n"," Accuracy: 0.9617\n"," F1 score: 0.9423\n"," Average training loss: 0.1698\n","Running Validation...\n","100% 560/560 [00:37<00:00, 14.83it/s]\n"," Accuracy: 0.7716\n"," F1 score: 0.6986\n","======== Epoch 4 / 10 ========\n","Training...\n","1680it [07:34,  3.69it/s]\n"," Accuracy: 0.9698\n"," F1 score: 0.9573\n"," Average training loss: 0.1391\n","Running Validation...\n","100% 560/560 [00:37<00:00, 14.87it/s]\n"," Accuracy: 0.7803\n"," F1 score: 0.7080\n","======== Epoch 5 / 10 ========\n","Training...\n","1680it [07:34,  3.70it/s]\n"," Accuracy: 0.9756\n"," F1 score: 0.9636\n"," Average training loss: 0.1135\n","Running Validation...\n","100% 560/560 [00:37<00:00, 14.86it/s]\n"," Accuracy: 0.7761\n"," F1 score: 0.7013\n","======== Epoch 6 / 10 ========\n","Training...\n","158it [00:43,  3.73it/s]Traceback (most recent call last):\n","  File \"train.py\", line 73, in <module>\n","    loss.backward()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/tensor.py\", line 245, in backward\n","    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 147, in backward\n","    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n","KeyboardInterrupt\n","158it [00:43,  3.62it/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NSKhnkF5CHt9","outputId":"de3489cb-1d88-452c-f904-363ee4427b30"},"source":["!python test.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-06-14 09:20:19.680898: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","Loading codes from transformers/PhoBERT_base_transformers/bpe.codes ...\n","Read 64000 codes from the codes file.\n","You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n","Some weights of the model checkpoint at transformers/PhoBERT_base_transformers/model.bin were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at transformers/PhoBERT_base_transformers/model.bin and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n","Some weights of the model checkpoint at transformers/PhoBERT_base_transformers/model.bin were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at transformers/PhoBERT_base_transformers/model.bin and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running Validation...\n","HBox(children=(FloatProgress(value=0.0, max=560.0), HTML(value='')))\n","\n"," Accuracy: 0.7759\n"," F1 score: 0.7139\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NaKktmnzDVKD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"854d58a4-9c6d-4910-a31f-c696a250f33c"},"source":["!python test.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-06-14 10:12:17.629954: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","Loading codes from transformers/PhoBERT_base_transformers/bpe.codes ...\n","Read 64000 codes from the codes file.\n","You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n","Some weights of the model checkpoint at transformers/PhoBERT_base_transformers/model.bin were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at transformers/PhoBERT_base_transformers/model.bin and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n","Some weights of the model checkpoint at transformers/PhoBERT_base_transformers/model.bin were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at transformers/PhoBERT_base_transformers/model.bin and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Nhập:trời ơi, ngon quá\n","Quá tốt\n","Nhập:tạm\n","Bình thường\n","Nhập:ổn\n","Tốt\n","Nhập:tệ quá\n","Tệ v\n","Nhập:không chấp nhận được\n","Tệ v\n","Nhập:Traceback (most recent call last):\n","  File \"test.py\", line 52, in <module>\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IP89IgkJxtWF"},"source":[""],"execution_count":null,"outputs":[]}]}